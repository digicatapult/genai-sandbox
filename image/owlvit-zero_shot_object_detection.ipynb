{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7469a4-5e07-440e-ae25-2b4c6cd47146",
   "metadata": {},
   "source": [
    "# OWL - Vit based zero shot object detection on images\n",
    "\n",
    "Here we are using a pretrained Vision transformer based model by google, [OWL-Vit](https://huggingface.co/docs/transformers/main/model_doc/owlvit) to explain zero shot image detection capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb787149-e7c6-4361-926d-3532b8735484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import transformers\n",
    "\n",
    "# To suppress some unwanted warnings that arises\n",
    "from transformers.utils import logging\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f482a63-0d22-448d-a8e5-375e9f3b6228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for image visualisation\n",
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d9dfc-6947-47e2-a2a2-1eb2aa4bf1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bboxes(image_url: str, predictions: list[dict]) -> None:\n",
    "    \"\"\"Utility function to visualise images and predictions.\n",
    "\n",
    "    Args:\n",
    "        image_url (str): Input image url\n",
    "        predictions (list[dict]): List of dictionaries conraining the predictions for the image\n",
    "                                   eg:[{'score': 0.29,\n",
    "                                      'label': 'car',\n",
    "                                      'box': {'xmin': 69, 'ymin': 188, 'xmax': 156, 'ymax': 291}},\n",
    "                                     {'score': 0.78,\n",
    "                                      'label': 'car',\n",
    "                                      'box': {'xmin': 496, 'ymin': 160, 'xmax': 525, 'ymax': 182}},\n",
    "                                     {'score': 0.18434,\n",
    "                                      'label': 'person',\n",
    "                                      'box': {'xmin': 399, 'ymin': 106, 'xmax': 769, 'ymax': 516}}]\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load image from URL\n",
    "    response = requests.get(image_url)\n",
    "    image = Image.open(io.BytesIO(response.content))\n",
    "\n",
    "    # Create a drawing context\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Load a font for text annotations\n",
    "    font_path = \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\"\n",
    "    font_size = 20\n",
    "    font = ImageFont.truetype(font_path, size=font_size)\n",
    "\n",
    "    # iteratively overlay all the detections, labels, and confidence scores on the image\n",
    "    for pred in predictions:\n",
    "        label = pred[\"label\"]\n",
    "        score = pred[\"score\"]\n",
    "        box = pred[\"box\"]\n",
    "        xmin, ymin, xmax, ymax = box[\"xmin\"], box[\"ymin\"], box[\"xmax\"], box[\"ymax\"]\n",
    "\n",
    "        # Draw rectangle\n",
    "        draw.rectangle([xmin, ymin, xmax, ymax], outline=\"red\", width=2)\n",
    "\n",
    "        # Add label and confidence score\n",
    "        text = f\"{label}: {score:.2f}\"\n",
    "        draw.text((xmin, ymin), text, fill=\"blue\", font=font)\n",
    "\n",
    "    # Display the image\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9763f4f8-ad33-4982-a2c1-a19d8ff5740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the id of the free GPU that you are intented to use\n",
    "# \"auto\" is not yet supported by this pipeline as of transformers v4.33.0\n",
    "# the model is small enough to load to a single GPU - consumes less than 6GB\n",
    "gpu_id = 0\n",
    "\n",
    "# model = \"google/owlvit-base-patch32\"\n",
    "model = \"google/owlvit-large-patch14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114938e1-884c-4c40-9299-ffc27c901c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    task=\"zero-shot-object-detection\",\n",
    "    model=model,\n",
    "    device=gpu_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8463f4e3-4f4f-4bab-85b6-7cac733b1f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "items_to_detect1 = [\"cat\", \"couch\", \"remote\", \"ear\", \"tail\"]\n",
    "\n",
    "result1 = pipeline(\n",
    "    url1,\n",
    "    candidate_labels=items_to_detect1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee762f-a34c-4a76-b758-80266ef61615",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_bboxes(url1, result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7fc57e-3a46-442e-a924-63e05ec4eb35",
   "metadata": {},
   "source": [
    "This model is able to detect custom `classes` or categories without being explicitely trained to do so.\n",
    "\n",
    "This is generally called [zero shot detection](https://huggingface.co/docs/transformers/main/tasks/zero_shot_object_detection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3fef4e-da94-4873-8d9e-533824869154",
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = \"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"\n",
    "items_to_detect2 = [\"bird\", \"eye\", \"beak\"]\n",
    "\n",
    "result2 = pipeline(\n",
    "    url2,\n",
    "    candidate_labels=items_to_detect2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5982824-2da1-4505-86af-7f356a532788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results - just to understand what is being returned from the pipeline\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9925300a-36d3-4fda-ada0-faf5b5f77a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_bboxes(url2, result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd5498e-f7d6-4b0c-97eb-8df1fe79f460",
   "metadata": {},
   "source": [
    "In case if you notice anything weird with the detections?\n",
    "\n",
    "The same objects getting detected multiple times but with slight difference in pixel locations can be easily eliminated with a post processing  technique called [Non Maximal Suppression](https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5e3323-8058-43a5-bd99-a258bc0ca5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
