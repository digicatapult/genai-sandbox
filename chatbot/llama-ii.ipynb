{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a21479-f3c6-4e86-864f-a8e59732f8c1",
   "metadata": {},
   "source": [
    "# LLama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5d960d-75f0-490c-b81d-876c688618aa",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "Select the model, config and inject the system prompt.\n",
    "\n",
    "Run the following cells: the cell **below**; the cells in the **Install & Stuff** section; the cells in the **Import** section; the cells in the **Load Model**, Tokenizer & Pipe; the cells in the **Set Up The UI** section and the **last** section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65fa7ad-29bc-4342-8461-e1ad7459641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select the language model\n",
    "model_id = \"TheBloke/Llama-2-7B-chat-GPTQ\"\n",
    "\n",
    "# Other available models are:\n",
    "# model_id = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "# model_id = \"TheBloke/Llama-2-70B-chat-GPTQ\"\n",
    "\n",
    "# Configuration\n",
    "runtimeFlag = \"cuda:0\"  # Run on GPU (you can't run GPTQ on cpu)\n",
    "cache_dir = None  # by default, don't set a cache directory?!\n",
    "scaling_factor = 1.0  # allows for a max sequence length of 16384*6 = 98304! Unfortunately, requires a V100 or A100 to have sufficient RAM.\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"You are a helpful assistant.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5408a57d-b657-410e-97ea-99546ffa26d4",
   "metadata": {},
   "source": [
    "## Install and Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee2c747-6592-4e23-b9fe-6a78a8a26d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you arerunning this notebook for the first time - uncomment and run the below line\n",
    "\n",
    "# !pip install optimum, auto-gptq, pdfminer-six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fea49b-db75-40a0-8efa-7d273b626ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8964e3f-6ad0-4e76-931d-fa6811e13184",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dd6ec9-246c-4f42-a88a-bf1d65f10a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from transformers import AutoTokenizer, TextStreamer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9690e8ce-74ba-4c37-a7f8-c1ec7ce699f3",
   "metadata": {},
   "source": [
    "## Load Model, Tokenizer & Pipe\n",
    "\n",
    "If you have this saved locally this is gonna go fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4567872-019c-4cd5-95bb-7fbffe55a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    # device_map=\"auto\",\n",
    "    # torch_dtype=torch.float16,\n",
    "    # rope_scaling = {\"type\": \"dynamic\", \"factor\": scaling_factor}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92528e2b-ab1e-456b-b2fc-5c50e68c5ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    # config=model_config,\n",
    "    # torch_dtype=torch.float16,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7869581-5381-48f0-9b3f-344d1a1136d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655334f1-bd17-4db3-997f-d5eced63409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890c94dd-8b50-4aa3-89d8-dd0543276c59",
   "metadata": {},
   "source": [
    "## Set Up The UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fa8653-4d68-4688-b678-80f84d38c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for the UI\n",
    "from IPython.display import display, HTML, clear_output, Markdown\n",
    "import textwrap, json\n",
    "import ipywidgets as widgets\n",
    "import re, time\n",
    "from pdfminer.high_level import extract_text\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e43843-0352-4cc0-bf4e-e4d5f9d496a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"  # B_INST, E_INST = \"Question: \", \"Answer: \"\n",
    "\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "SYSTEM_PROMPT = DEFAULT_SYSTEM_PROMPT\n",
    "\n",
    "# max_doc_length = 50\n",
    "max_context = 16384  # int(model.config.max_position_embeddings*scaling_factor)\n",
    "max_doc_length = int(0.75 * max_context)  # max doc length is 75% of the context length\n",
    "max_doc_words = int(max_doc_length)\n",
    "max_prompt_len = int(0.85 * max_context) / 1  # Not sure if this is needed\n",
    "max_gen_len = int(0.10 * max_prompt_len) / 1  # Not sure if this is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a6b969-a6b9-4588-ba42-24e6f414ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(dialogs, temperature=0.01, top_p=0.9, logprobs=False):\n",
    "    \"\"\"Generate the next response to the conversation so far.\"\"\"\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "    prompt = \"\"  # prompt_tokens = []\n",
    "    for dialog in dialogs:\n",
    "        if dialog[0][\"role\"] != \"system\":\n",
    "            dialog = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": SYSTEM_PROMPT,\n",
    "                }\n",
    "            ] + dialog\n",
    "\n",
    "        prompt += f\"[INST] <<SYS>>\\\\n{dialog[0]['content'].strip()}\\\\n<</SYS>>\\\\n\\\\n{dialog[1]['content'].strip()} [/INST] { '' if len(dialog) <= 2 else dialog[2]['content'] }\"\n",
    "        for i in range(3, len(dialog), 2):\n",
    "            prompt += f\"<s>[INST] {dialog[i]['content']} [/INST] { '' if i+1 >= len(dialog) else ' '+dialog[i+1]['content']+' </s>' }\"\n",
    "\n",
    "    res = pipe(prompt)\n",
    "    new_assistant_response_all = res[0][\"generated_text\"]\n",
    "    new_assistant_response = new_assistant_response_all.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "    return new_assistant_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23e801b-bf56-440e-8f6c-1d4e609b74af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_wrapped(text):\n",
    "    \"\"\"Pretty print the response generated.\"\"\"\n",
    "    code_pattern = r\"```(.+?)```\"\n",
    "    matches = list(re.finditer(code_pattern, text, re.DOTALL))\n",
    "\n",
    "    if not matches:\n",
    "        # If there are no code blocks, display the entire text as Markdown\n",
    "        display(Markdown(text))\n",
    "        return\n",
    "\n",
    "    start = 0\n",
    "    for match in matches:\n",
    "        # Display the text before the code block as Markdown\n",
    "        before_code = text[start : match.start()].strip()\n",
    "        if before_code:\n",
    "            display(Markdown(before_code))\n",
    "\n",
    "        # Display the code block\n",
    "        code = match.group(0).strip()  # Extract code block\n",
    "        display(Markdown(code))  # Display code block\n",
    "\n",
    "        start = match.end()\n",
    "\n",
    "    # Display the text after the last code block as Markdown\n",
    "    after_code = text[start:].strip()  # Text after the last code block\n",
    "    if after_code:\n",
    "        display(Markdown(after_code))\n",
    "\n",
    "\n",
    "dialog_history = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "\n",
    "button = widgets.Button(description=\"Send\", style={\"button_color\": \"#ff6e00\"})\n",
    "upload_button = widgets.Button(description=\"Upload .txt or .pdf\")\n",
    "text = widgets.Textarea(layout=widgets.Layout(width=\"800px\"))\n",
    "\n",
    "output_log = widgets.Output()\n",
    "\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    \"\"\"Action on button click.\"\"\"\n",
    "    user_input = text.value\n",
    "    dialog_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    text.value = \"\"\n",
    "\n",
    "    # Change button description and color, and disable it\n",
    "    button.description = \"Processing...\"\n",
    "    button.style.button_color = (\n",
    "        \"#cc2200\"  # Use hex color codes for better color choices\n",
    "    )\n",
    "    button.disabled = True  # Disable the button when processing\n",
    "\n",
    "    with output_log:\n",
    "        clear_output()\n",
    "        for message in dialog_history:\n",
    "            print_wrapped(f'**{message[\"role\"].capitalize()}**: {message[\"content\"]}\\n')\n",
    "\n",
    "    assistant_response = generate_response([dialog_history])\n",
    "\n",
    "    # Re-enable the button, reset description and color after processing\n",
    "    button.description = \"Send\"\n",
    "    button.style.button_color = \"#ff6e00\"\n",
    "    button.disabled = False\n",
    "\n",
    "    dialog_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "    with output_log:\n",
    "        clear_output()\n",
    "        for message in dialog_history:\n",
    "            print_wrapped(f'**{message[\"role\"].capitalize()}**: {message[\"content\"]}\\n')\n",
    "\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "# Create an output widget for alerts\n",
    "alert_out = widgets.Output()\n",
    "\n",
    "clear_button = widgets.Button(description=\"Clear Chat\")\n",
    "text = widgets.Textarea(layout=widgets.Layout(width=\"800px\"))\n",
    "\n",
    "\n",
    "def on_clear_button_clicked(b):\n",
    "    \"\"\"Action on clear button click.\"\"\"\n",
    "    # Clear the dialog history\n",
    "    dialog_history.clear()\n",
    "    # Add back the initial system prompt\n",
    "    dialog_history.append({\"role\": \"system\", \"content\": SYSTEM_PROMPT})\n",
    "    # Clear the output log\n",
    "    with output_log:\n",
    "        clear_output()\n",
    "\n",
    "\n",
    "clear_button.on_click(on_clear_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c8b0f-4f03-40c1-91d9-2fbbedf4f249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from ipywidgets import HBox, VBox\n",
    "\n",
    "# Create the title with HTML\n",
    "title = f\"<h1 style='color: #ff6e00;'>Llama 2 Base Model 🦙</h1> <p>( Max context of: {'max_context'} or {'max_doc_words'} (TBD) )</p>\"\n",
    "\n",
    "# Assuming that output_log, alert_out, and text are other widgets or display elements...\n",
    "first_row = HBox([button, clear_button])  # Arrange these buttons horizontally\n",
    "\n",
    "# Arrange the two rows of buttons and other display elements vertically\n",
    "layout = VBox([output_log, alert_out, text, first_row])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc0d069-9ae7-495c-ba7b-8cd0d11a7d7e",
   "metadata": {},
   "source": [
    "## Chat with Llama 2\n",
    "\n",
    "The below cell creates a chat interface within the notebook and you can start chatting with Llama2 through the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ad705-f3bd-4b4a-a164-100b9b3f5b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(title))  # Use HTML function to display the title\n",
    "display(layout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
