{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e505db7b-20dc-451e-b335-48d2ad3f2b0e",
   "metadata": {},
   "source": [
    "## DialoGPT conversational pipeline\n",
    "\n",
    "This notebook provides [DialoGPT by Microsoft](https://huggingface.co/microsoft/DialoGPT-medium) playground for getting familiar with huggingface conversational pipeline.\n",
    "\n",
    "The models that conversational pipeline supports are models that have been fine-tuned on a multi-turn conversational task (see https://huggingface.co/models?filter=conversational for a list of updated Conversational models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8972d5-c94a-4e0b-a08a-061e741d09d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1296b2c5-37ae-4144-88e3-45c3c65f69b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, Conversation\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e3cd4-489d-4c50-b17e-13cfc2d04144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To suppress some unwanted misleading warnings that comes up with the recent version of conversation models\n",
    "from transformers.utils import logging\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4300afc-bf34-4647-b998-b16b30bd1b14",
   "metadata": {},
   "source": [
    "#### Modify the below cell to change the GPU and/or model to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39a4f73-0246-4da0-89ec-f66025522542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the id of the free GPU that you are intented to use\n",
    "# Use \"auto\" instead of specifying integer id for gpu_id if unsure\n",
    "gpu_id = \"auto\"\n",
    "\n",
    "model = \"microsoft/DialoGPT-medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb5e3e-04bb-4b4e-a778-1b7d49252c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a6fadd-e71d-4088-baf9-ac3fb81cdb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a conversational pipeline\n",
    "pipeline = transformers.pipeline(\n",
    "    \"conversational\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=gpu_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e5a91d-2736-44f9-ad1b-152294ef21cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise a conversation class instance\n",
    "conversation = Conversation()\n",
    "\n",
    "\n",
    "def chat(prompt: str, print_entire_conversation: bool = True) -> None:\n",
    "    \"\"\"A chat api to get continuous conversation.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): Input text (prompt) to the model\n",
    "        print_entire_conversation (bool): To print the entire conversation or only the last response\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    #TODO: It is a bad practice to use `global` variable. Wrap this function into a class\n",
    "           and declare `conversation` as an attribute.\n",
    "    \"\"\"\n",
    "    global conversation\n",
    "\n",
    "    # attach the prompt to the conversation\n",
    "    conversation.add_user_input(prompt)\n",
    "\n",
    "    # get the response from model to the prompt\n",
    "    conversation = pipeline(conversation)\n",
    "\n",
    "    # print the response\n",
    "    if print_entire_conversation:\n",
    "        print(conversation)\n",
    "    else:\n",
    "        print(conversation.generated_responses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02edfa61-1830-41de-b8b9-d88690c5d952",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"Any recommendation for a friends movie night?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d1290-1a4e-4e0f-b012-81c1232d80e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"Is it an action movie?\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c524fdb-0b27-446f-bc03-91552e533fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"Any other sci-fi movie recommendation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787bd2a5-2173-4d39-a0b3-67aaf139df70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76d79be6-74e1-4981-b62c-0b0078ddea2e",
   "metadata": {},
   "source": [
    "Try continuing the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930d0298-f25a-4cde-8661-4df34396a9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
